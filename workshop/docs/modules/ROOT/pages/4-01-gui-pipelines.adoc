= 4.1 Workbench Pipeline Editor

include::_attributes.adoc[]

== Automating with Pipelines

As you saw in previous sections, you can use notebooks to train and save your models.  However, you can also automate these tasks using *Red Hat Data Science Pipelines*.  Pipelines offer a way to automate the execution of multiple notebooks, python code, and to schedule them to run.  This way, you can execute long training jobs, or retrain your models on a schedule without having to manually run them in a notebook.

In this first section, we will create a very simple pipeline.  We want to train a model and then save it to S3 storage and we'll use the exact same notebooks from the previous sections to do so.

We will start with a GUI editor right from your workbench.  The completed pipeline will look like `6 Train Save.pipeline`.

== Creating a New Pipeline

Let's create a new pipeline.  Start from you workbench.

image::pipelines/wb-pipeline-launcher.png[Pipeline Buttons]

* Open the *Launcher* from the *+* button, if the launcher is not visible

image::pipelines/wb-launcher-button.png[Launcher Button]

* Click the *Pipeline Editor* to create a new blank pipeline

image::pipelines/wb-pipeline-editor-button.png[Pipeline Editor Button]


== Setting Pipeline Properties

Great.  You've created a blank pipeline!  Let's set some properties for the pipeline and any nodes that are created.  We're going to set the default runtime image which will be used to run your notebooks or python code unless otherwise set.

* Click the *Open Panel* button in the top right corner of the pipeline editor.

image::pipelines/wb-pipeline-panel-button.png[Open Panel Button]

image::pipelines/wb-pipeline-panel-button-loc.png[Open Panel]

* Select the *Pipeline Properties* tab

image::pipelines/wb-pipeline-properties-tab.png[Pipeline Properties Tab]

* In the open pipeline properties panel, scroll down to *Generic Node Defaults* and *Runtime Image*.  Set the value to `Tensorflow with Cuda and Python 3.9 (UBI 9)`

image::pipelines/wb-pipeline-runtime-image.png[Pipeline Runtime Image0]

== Creating Nodes

So now that the pipeline is set up.  Let's add some steps, or *nodes* in our pipeline.  Our two nodes will use notebooks `1_experiment_train.ipynb` and `2_save_model.ipynb`.

* Click in the left filebrowser, and drag the notebooks onto the pipeline canvas.

image::pipelines/wb-pipeline-drag-drop.png[ Drag and Drop Notebooks]

* Connect the output port of `1_experiment_train.ipynb` to the input port of `2_save_model.ipynb` by drawing a line from one to another.  Click and drag from the right point on node 1 to the left point on node 2.

image::pipelines/wb-pipeline-connect-nodes.png[Connect Nodes]

== Setting Node Properties

Now that our pipeline has been constructed, we need to set some properties to include and pass files between nodes.  First, let's select node 1, `1_experiment_train.ipynb`.

* Click the node, `1_experiment_train.ipynb` to select it.  It will be outlined in blue.

image::pipelines/wb-pipeline-node-1.png[Select Node 1]

* Click on the *Node Properties* tab in the properties panel.

Now we can add the *File Dependencies* to include the data to train our model.  If we don't set this, the file won't be included in the node when it runs and the training job would fail.

* Scroll down and click *Add*` under *File Dependencies*.

image::pipelines/wb-pipeline-node-1-file-dep.png[Add File Dependency]

* Set the value to `data/card_transdata.csv`.

image::pipelines/wb-pipeline-node-1-file-dep-form.png[Set File Dependency Value]

==  Node Outputs 

In node 1, we'll be creating `models/fraud/model.onnx` and in node 2, we want to upload that file to S3.  We need to set the output of node 1 so that it will be available for node 2.

* Select node 1

* Select the *Node Properties* tab

* Scroll down and click *Add*` under *Output Files*.

image::pipelines/wb-pipeline-node-1-add-output.png[]

* Set the value to `models/fraud/model.onnx`.

image::pipelines/wb-pipeline-node-1-file-output-form.png[Set File Dependency Value]

Now let's set the input of node 2.

* Select node 2

* Select the *Node Properties* tab

* Scroll down and click *Add*` under *Output Files*.

image::pipelines/wb-pipeline-node-1-add-output.png[]

* Set the value to `models/fraud/model.onnx`.

image::pipelines/wb-pipeline-node-1-file-output-form.png[Set File Dependency Value]


== Node Properties for Data Connections

Node 2 will be uploading the model to S3.  We need to set the S3 bucket and keys for the file.  We'll be using the secret created by the Data Connection `My Storage` in the project setup.  The secret is called `aws-connection-my-storage` as you can see by hovering over the connection info *?* in the *Data Connections* tab.  If you called your data connection something other than `My Storage`, you'll need to use look up the secret name.

image::pipelines/dsp-dc-secret-name.png[My Storage Secret Name]

We can use this secret in our pipeline nodes without having to save the information in our pipeline code.  This is important if you would like to save these pipelines to source control without any secret keys.

The secret `aws-connection-my-storage` has the following fields:

```
AWS_ACCESS_KEY_ID
AWS_DEFAULT_REGION
AWS_S3_BUCKET
AWS_S3_ENDPOINT
AWS_SECRET_ACCESS_KEY
```

First let's examine the node properties for node 2.

* Select node 2

* Select the *Node Properties* tab

Here', you'll see some under *Additional Properties* that *Environment Variables* that have been pre-filled out.  The editor inferred you'd need them from the notebook code.  


Since we don't want to save the value in our pipelines, we'll remove all of these environment variables.

* Click *Remove* for each of the environment variables.

image::pipelines/wb-pipeline-node-remove-env-var.png[Remove Env Var]

Now that your node is pipeline is clean, let's add the S3 bucket and keys using the Kubernetes secret.

* Click *Add* under *Kubernetes Secrets*.

image::pipelines/wb-pipeline-add-kube-secret.png[Add Kube Secret]

* Edit the form to set the values for the S3 bucket and keys.

image::pipelines/wb-pipeline-add-kube-secret.png[Add Kube Secret]

* Fill out the form with the following information
** *Environment Variable*: `AWS_ACCESS_KEY_ID`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_ACCESS_KEY_ID`

image::pipelines/wb-pipeline-kube-secret-form.png[Secret Form]

Repeat the procedure for the other keys.

* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_SECRET_ACCESS_KEY`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_SECRET_ACCESS_KEY`
* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_S3_ENDPOINT`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_S3_ENDPOINT`
* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_DEFAULT_REGION`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_DEFAULT_REGION`
* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_S3_BUCKET`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_S3_BUCKET`

* At this point you can *Save* and *Rename* the `.pipeline` file to save your changes.

== Running the Pipeline 

Now it's time to upload the pipeline on the cluster itself and run it.  You can do so directly from the pipeline editor.  You can use your own newly created pipeline for this or `6 Train Save.pipeline`.

* Click the play button in the toolbar of the pipeline editor.

image::pipelines/wb-pipeline-run-button.png[Pipeline Run Button]

* Enter a name for your pipeline
* Verify the *Runtime Configuration:* is set to `Data Science Pipeline`.  
* Click *OK*

NOTE: If `Data Science Pipeline` is not available as a runtime configuration, you may have created your notebook before the pipeline server was available.  You can restart your notebook after the pipeline server has been created in your data science project.

image::pipelines/wb-pipeline-run.png[Pipeline Run]

* Return to your data science project and expand the newly created pipeline.

image::pipelines/dsp-pipeline-complete.png[]

From here you can click on the pipeline or the pipeline run and view the pipeline run in progress.

image::pipelines/pipeline-run-complete.png[]

The result should be a `models/fraud/model.onnx` file in your S3 bucket which you can serve, just like we manually did in the previous section.