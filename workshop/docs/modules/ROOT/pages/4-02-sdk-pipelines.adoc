= 4.2 Pipelines in Python

include::_attributes.adoc[]

== The SDK

While we've created a simple pipeline using the GUI pipeline editor, it's often desirable to create pipelines using code that can be version controlled and shared with others. The https://github.com/kubeflow/kfp-tekton[kfp-tekton] SDK provides a Python API for creating pipelines. The SDK is available as a Python package that can be installed using `pip install kfp-tekton`.  With this package, you can use Python code to create a pipeline and then compile it to Tekton YAML.  Then we can import that YAML into OpenShift AI.

If you'd like to browse the code to generate, you can find it in the `pipeline` directory in you `fraud-detection-notebooks` project you are using in your notebook.  Feel free to peruse the code.

* `7_get_data_train_upload.py` is the main pipeline code.
* `get_data.py`, `train_model.py`, and `upload.py` are the three components of the pipeline.
* `build.sh` is a script that builds the pipeline and creates `7_get_data_train_upload.yaml`, which is what we'll need to upload.

We won't be delving into the SDK for this workshop, and have provided you with the yaml file, `7_get_data_train_upload.yaml` in the git repo with your notebooks.  You can download it from

++++
<a href="https://github.com/cfchase/fraud-detection-notebooks/raw/main/7_get_data_train_upload.yaml" download>https://github.com/cfchase/fraud-detection-notebooks/raw/main/7_get_data_train_upload.yaml</a>
++++


// link:https://github.com/cfchase/fraud-detection-notebooks/raw/main/7_get_data_train_upload.yaml[Download Pipeline YAML, window=_blank, download]

Or, you can download it directly from your workbench:

image::pipelines/wb-download.png[Download Pipeline YAML]

Now you can upload `7_get_data_train_upload.yaml` to OpenShift AI.  Start from the OpenShift AI dashboard.

* Click *Import pipeline* in the *Pipelines* section.

image::pipelines/dsp-pipeline-import.png[]

* Fill out the *Pipeline name* and *Pipeline description*

* Click *Upload* and select `7_get_data_train_upload.yaml` from your local files to upload the pipeline.

image::pipelines/dsp-pipline-import-upload.png[]

* Click *Import pipeline* to import and save the pipeline.

image::pipelines/dsp-pipline-import-button.png[]

As you can tell, no runs have been triggered yet.

* Click *Create run* either under the *Actions* menu or in the expand pipeline item.

* Fill out the *Name* on the form. You can leave other fields as default.

image::pipelines/pipeline-create-run-form.png[Create Pipeline Run Form]

* Click *Create* to create the run.

image::pipelines/pipeline-run-create-button.png[Create Pipeline Run Button]

This will kick off a new run immediately and send you to the run details page.

image::pipelines/pipeline-run-in-progress.png[]

There you have it.  A pipeline created in Python running in OpenShift AI.





